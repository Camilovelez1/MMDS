{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPt5q27L5557"
      },
      "source": [
        "# ST1803 Usando PySpark\n",
        "\n",
        "Vamos a configurar el uso de PySpark en el cuaderno (dirigido principalmente para el uso en Google Colab) y haremos algunos ejemplos de calentamiento.\n",
        "\n",
        "El código de configuración tomado del repositorio https://github.com/groda/big_data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-YhEpP_Ds-"
      },
      "source": [
        "## Instalar Spark\n",
        "Usaremos el manejador de paquetes `pipenv` para instalar `pyspark`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q74xcMdqfbJB",
        "outputId": "7a0364d9-a202-48c4-c748-5e8bc183bd95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling pyspark~=\u001b[0m\u001b[1;36m3.5\u001b[0m\u001b[1;32m.\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving pyspark~=\u001b[1;36m3.5\u001b[0m.\u001b[1;36m0\u001b[0m\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing pyspark...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1m04efcd\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n"
          ]
        }
      ],
      "source": [
        "!pipenv install pyspark~=3.5.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hknPATlqgiXd"
      },
      "source": [
        "Revisar si se tiene Java 8 o posterior. En Colab tenemos el último Java (11) pero en otros ambientes deberá ser instalado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def is_java_installed():\n",
        "    java_path = shutil.which(\"java\")\n",
        "    if java_path:\n",
        "        os.environ['JAVA_HOME'] = os.path.realpath(java_path).split('/bin')[0]\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Uso de la función\n",
        "if is_java_installed():\n",
        "    print(\"✅ Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n",
        "else:\n",
        "    print(\"❌ Java is not installed. Please install Java.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWCBODIski2x"
      },
      "source": [
        "## Ejemplo1: Hello World\n",
        "Vamos a empezar con una aplicación que:\n",
        "\n",
        "\n",
        "*   Comience una sesión de Spark llamada `spark`\n",
        "*   Imprima \"Hello, World!\"\n",
        "*   Cierre la sesión de Spark.\n",
        "\n",
        "Esta sería una aplicación auto-contenida (ver https://spark.apache.org/docs/latest/quick-start.html#self-contained-applications).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eevSjgIEoaNh",
        "outputId": "c8e92d4b-ea8a-4f4b-bf6d-c19415d7c7d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing HelloWorld.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile HelloWorld.py\n",
        "\"\"\"HelloWorld.py\"\"\"\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Hello World\").getOrCreate()\n",
        "\n",
        "print(\"Hello, World!\")\n",
        "\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzN0YKnFrPia"
      },
      "source": [
        "Para ejecutar una aplicación en Spark se puede utilizar directamente Python, pero si se quiere utilizar toda la configuración de Spark debe usar el script `spark-submit`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HKYobzFolpR",
        "outputId": "b1fa8d1a-ce20-4f64-e6eb-7ce6f3be4e95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/02/20 10:42:12 WARN Utils: Your hostname, Camilo resolves to a loopback address: 127.0.1.1; using 172.29.121.68 instead (on interface eth0)\n",
            "24/02/20 10:42:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "24/02/20 10:42:14 INFO SparkContext: Running Spark version 3.5.0\n",
            "24/02/20 10:42:14 INFO SparkContext: OS info Linux, 5.15.133.1-microsoft-standard-WSL2, amd64\n",
            "24/02/20 10:42:14 INFO SparkContext: Java version 11.0.21\n",
            "24/02/20 10:42:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/02/20 10:42:14 INFO ResourceUtils: ==============================================================\n",
            "24/02/20 10:42:14 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "24/02/20 10:42:14 INFO ResourceUtils: ==============================================================\n",
            "24/02/20 10:42:14 INFO SparkContext: Submitted application: Hello World\n",
            "24/02/20 10:42:14 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "24/02/20 10:42:14 INFO ResourceProfile: Limiting resource is cpu\n",
            "24/02/20 10:42:14 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "24/02/20 10:42:14 INFO SecurityManager: Changing view acls to: camilo\n",
            "24/02/20 10:42:14 INFO SecurityManager: Changing modify acls to: camilo\n",
            "24/02/20 10:42:14 INFO SecurityManager: Changing view acls groups to: \n",
            "24/02/20 10:42:14 INFO SecurityManager: Changing modify acls groups to: \n",
            "24/02/20 10:42:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: camilo; groups with view permissions: EMPTY; users with modify permissions: camilo; groups with modify permissions: EMPTY\n",
            "24/02/20 10:42:14 INFO Utils: Successfully started service 'sparkDriver' on port 40097.\n",
            "24/02/20 10:42:14 INFO SparkEnv: Registering MapOutputTracker\n",
            "24/02/20 10:42:14 INFO SparkEnv: Registering BlockManagerMaster\n",
            "24/02/20 10:42:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "24/02/20 10:42:14 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "24/02/20 10:42:14 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "24/02/20 10:42:14 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-adb0a52e-1d3e-4676-884f-f9cfe4dbe936\n",
            "24/02/20 10:42:14 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "24/02/20 10:42:14 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "24/02/20 10:42:14 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "24/02/20 10:42:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "24/02/20 10:42:15 INFO Executor: Starting executor ID driver on host 172.29.121.68\n",
            "24/02/20 10:42:15 INFO Executor: OS info Linux, 5.15.133.1-microsoft-standard-WSL2, amd64\n",
            "24/02/20 10:42:15 INFO Executor: Java version 11.0.21\n",
            "24/02/20 10:42:15 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "24/02/20 10:42:15 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@77b95588 for default.\n",
            "24/02/20 10:42:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38735.\n",
            "24/02/20 10:42:15 INFO NettyBlockTransferService: Server created on 172.29.121.68:38735\n",
            "24/02/20 10:42:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "24/02/20 10:42:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.29.121.68, 38735, None)\n",
            "24/02/20 10:42:15 INFO BlockManagerMasterEndpoint: Registering block manager 172.29.121.68:38735 with 434.4 MiB RAM, BlockManagerId(driver, 172.29.121.68, 38735, None)\n",
            "24/02/20 10:42:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.29.121.68, 38735, None)\n",
            "24/02/20 10:42:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.29.121.68, 38735, None)\n",
            "Hello, World!\n",
            "24/02/20 10:42:15 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "24/02/20 10:42:15 INFO SparkUI: Stopped Spark web UI at http://172.29.121.68:4040\n",
            "24/02/20 10:42:15 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "24/02/20 10:42:15 INFO MemoryStore: MemoryStore cleared\n",
            "24/02/20 10:42:15 INFO BlockManager: BlockManager stopped\n",
            "24/02/20 10:42:15 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "24/02/20 10:42:15 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "24/02/20 10:42:15 INFO SparkContext: Successfully stopped SparkContext\n",
            "24/02/20 10:42:15 INFO ShutdownHookManager: Shutdown hook called\n",
            "24/02/20 10:42:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-49a4aad3-0a4a-4167-b706-7cc6a9546f42\n",
            "24/02/20 10:42:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-d2ebf788-b883-4e71-81f6-fac64bdafc74\n",
            "24/02/20 10:42:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-d2ebf788-b883-4e71-81f6-fac64bdafc74/pyspark-66e34651-dca0-4c89-a111-59d0df9ac602\n"
          ]
        }
      ],
      "source": [
        "!spark-submit HelloWorld.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brBqIRlJrd0A"
      },
      "source": [
        "Todo el texto adicional de \"Hello, World!\" es debido a mensajes logs de la plataforma. Si se quiere tener estos logs aparte los puedo llegar a un archivo (por defecto van al stream estándar de errores)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQP0a0CXoyQY",
        "outputId": "a7299d3e-d4c4-478f-f0aa-3d8d1331f304"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, World!\n"
          ]
        }
      ],
      "source": [
        "!spark-submit HelloWorld.py 2>log.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm4ijR2hrwHt"
      },
      "source": [
        "Ahora tengo los logs separados, que puedo revisar en el archivo `log.txt`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbg5z3eOr40g",
        "outputId": "1fe15727-010e-4e56-cd33-8921dabcf8d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/02/20 10:42:44 WARN Utils: Your hostname, Camilo resolves to a loopback address: 127.0.1.1; using 172.29.121.68 instead (on interface eth0)\n",
            "24/02/20 10:42:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "24/02/20 10:42:48 INFO SparkContext: Running Spark version 3.5.0\n",
            "24/02/20 10:42:48 INFO SparkContext: OS info Linux, 5.15.133.1-microsoft-standard-WSL2, amd64\n",
            "24/02/20 10:42:48 INFO SparkContext: Java version 11.0.21\n",
            "24/02/20 10:42:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/02/20 10:42:48 INFO ResourceUtils: ==============================================================\n",
            "24/02/20 10:42:48 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "24/02/20 10:42:48 INFO ResourceUtils: ==============================================================\n",
            "24/02/20 10:42:48 INFO SparkContext: Submitted application: Hello World\n",
            "24/02/20 10:42:48 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "24/02/20 10:42:48 INFO ResourceProfile: Limiting resource is cpu\n",
            "24/02/20 10:42:48 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "24/02/20 10:42:48 INFO SecurityManager: Changing view acls to: camilo\n",
            "24/02/20 10:42:48 INFO SecurityManager: Changing modify acls to: camilo\n",
            "24/02/20 10:42:48 INFO SecurityManager: Changing view acls groups to: \n",
            "24/02/20 10:42:48 INFO SecurityManager: Changing modify acls groups to: \n",
            "24/02/20 10:42:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: camilo; groups with view permissions: EMPTY; users with modify permissions: camilo; groups with modify permissions: EMPTY\n",
            "24/02/20 10:42:48 INFO Utils: Successfully started service 'sparkDriver' on port 46297.\n",
            "24/02/20 10:42:48 INFO SparkEnv: Registering MapOutputTracker\n",
            "24/02/20 10:42:48 INFO SparkEnv: Registering BlockManagerMaster\n",
            "24/02/20 10:42:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "24/02/20 10:42:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "24/02/20 10:42:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "24/02/20 10:42:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8fea3989-2cd6-4d3f-9230-060a50d153e2\n",
            "24/02/20 10:42:48 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "24/02/20 10:42:48 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "24/02/20 10:42:48 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "24/02/20 10:42:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "24/02/20 10:42:48 INFO Executor: Starting executor ID driver on host 172.29.121.68\n",
            "24/02/20 10:42:48 INFO Executor: OS info Linux, 5.15.133.1-microsoft-standard-WSL2, amd64\n",
            "24/02/20 10:42:48 INFO Executor: Java version 11.0.21\n",
            "24/02/20 10:42:48 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "24/02/20 10:42:48 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@3053b3f2 for default.\n",
            "24/02/20 10:42:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43357.\n",
            "24/02/20 10:42:48 INFO NettyBlockTransferService: Server created on 172.29.121.68:43357\n",
            "24/02/20 10:42:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "24/02/20 10:42:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.29.121.68, 43357, None)\n",
            "24/02/20 10:42:48 INFO BlockManagerMasterEndpoint: Registering block manager 172.29.121.68:43357 with 434.4 MiB RAM, BlockManagerId(driver, 172.29.121.68, 43357, None)\n",
            "24/02/20 10:42:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.29.121.68, 43357, None)\n",
            "24/02/20 10:42:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.29.121.68, 43357, None)\n",
            "24/02/20 10:42:49 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "24/02/20 10:42:49 INFO SparkUI: Stopped Spark web UI at http://172.29.121.68:4040\n",
            "24/02/20 10:42:49 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "24/02/20 10:42:49 INFO MemoryStore: MemoryStore cleared\n",
            "24/02/20 10:42:49 INFO BlockManager: BlockManager stopped\n",
            "24/02/20 10:42:49 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "24/02/20 10:42:49 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "24/02/20 10:42:49 INFO SparkContext: Successfully stopped SparkContext\n",
            "24/02/20 10:42:49 INFO ShutdownHookManager: Shutdown hook called\n",
            "24/02/20 10:42:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-b056808f-19b4-47d9-8b0e-2f61d6de6a63/pyspark-730f7d56-fc4f-4906-a17e-fd2f90822c3f\n",
            "24/02/20 10:42:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-062f4831-56a0-4578-a10f-9fdc148c5850\n",
            "24/02/20 10:42:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-b056808f-19b4-47d9-8b0e-2f61d6de6a63\n"
          ]
        }
      ],
      "source": [
        "!cat log.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjJFIEHossMN"
      },
      "source": [
        "Pero ejecutar la aplicación se sintió muy lenta, la razón de la mayoría de la lentitud tiene que ver con la máquina virtual de Java (JVM en inglés), la cual debe ejecutarse y luego el motor de Spark se ejecuta sobre esta. Veamos cuánto toma sólo la aplicación en ejecutarse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siRB48IWtLhH",
        "outputId": "9b28c54e-8fab-4c92-b8e0-db70a7e7edba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, World!\n",
            "CPU times: user 83.6 ms, sys: 9.69 ms, total: 93.3 ms\n",
            "Wall time: 4.71 s\n"
          ]
        }
      ],
      "source": [
        "%time !spark-submit HelloWorld.py 2>log.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_mrRsz8t9Jt"
      },
      "source": [
        "## Ejemplos en PySpark\n",
        "PySpark viene con muchos ejemplos en su instalación, para encontrarlos hay que saber dónde quedó instalado PySpark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAEurL1DuGZS",
        "outputId": "c349d0e1-d6b6-44ab-ede3-67dd60826223"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: pyspark\n",
            "Version: 3.5.0\n",
            "Summary: Apache Spark Python API\n",
            "Home-page: https://github.com/apache/spark/tree/master/python\n",
            "Author: Spark Developers\n",
            "Author-email: dev@spark.apache.org\n",
            "License: http://www.apache.org/licenses/LICENSE-2.0\n",
            "Location: /home/camilo/.local/share/virtualenvs/MMDS-OQFbRnK5/lib/python3.11/site-packages\n",
            "Requires: py4j\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20ilbckEuI0i"
      },
      "source": [
        "/usr/local/lib/python3.10/dist-packages es donde quedó instalado, debemos buscar la carpeta `examples`. Otra forma de saber donde está instalado es usar el archivo `find_spark_home.py` y usarlo para crear una variable de ambiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9POsJU1auzgn",
        "outputId": "62a63846-81c1-4b41-9922-7441bdcce261"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/camilo/.local/share/virtualenvs/MMDS-OQFbRnK5/lib/python3.11/site-packages/pyspark\n"
          ]
        }
      ],
      "source": [
        "!find_spark_home.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvPktN_dvCZr",
        "outputId": "de3c4d92-1306-4751-9a25-09d5492e53d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Carpeta de PySpark en: /home/camilo/.local/share/virtualenvs/MMDS-OQFbRnK5/lib/python3.11/site-packages/pyspark\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pyspark_folder = subprocess.run([\"find_spark_home.py\"], capture_output=True, text=True)\n",
        "print(\"Carpeta de PySpark en:\", pyspark_folder.stdout)\n",
        "# Resultado en una variable de entorno\n",
        "os.environ['SPARK_HOME'] = pyspark_folder.stdout.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMfjPMyfviJf",
        "outputId": "8cc7ed24-23c7-4ffd-e5ad-9f67d6cbb16a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__init__.py\t    install.py\t\t      sbin/\n",
            "__pycache__/\t    instrumentation_utils.py  serializers.py\n",
            "_globals.py\t    jars/\t\t      shell.py\n",
            "_typing.pyi\t    java_gateway.py\t      shuffle.py\n",
            "accumulators.py     join.py\t\t      sql/\n",
            "bin/\t\t    licenses/\t\t      statcounter.py\n",
            "broadcast.py\t    ml/\t\t\t      status.py\n",
            "cloudpickle/\t    mllib/\t\t      storagelevel.py\n",
            "conf.py\t\t    pandas/\t\t      streaming/\n",
            "context.py\t    profiler.py\t\t      taskcontext.py\n",
            "daemon.py\t    py.typed\t\t      testing/\n",
            "data/\t\t    python/\t\t      traceback_utils.py\n",
            "errors/\t\t    rdd.py\t\t      util.py\n",
            "examples/\t    rddsampler.py\t      version.py\n",
            "files.py\t    resource/\t\t      worker.py\n",
            "find_spark_home.py  resultiterable.py\t      worker_util.py\n"
          ]
        }
      ],
      "source": [
        "!ls -p $SPARK_HOME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7s1tYiEv3CO"
      },
      "source": [
        "Si estamos en Ubuntu podemos instalar una herramienta para ver mejor las carpetas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaRL-H6ZwCe0",
        "outputId": "164b08fe-6a4c-4ae1-b740-e43f3a3d463e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sudo] password for camilo: \n"
          ]
        }
      ],
      "source": [
        "#!apt install tree\n",
        "#!sudo apt install tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ptkrt6EzwIrn",
        "outputId": "f150b119-a7f5-4ebe-f7cf-7fdc10bcda4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[01;34m/home/camilo/.local/share/virtualenvs/MMDS-OQFbRnK5/lib/python3.11/site-packages/pyspark/examples\u001b[0m\n",
            "└── \u001b[01;34msrc\u001b[0m\n",
            "    └── \u001b[01;34mmain\u001b[0m\n",
            "        └── \u001b[01;34mpython\u001b[0m\n",
            "            ├── __init__.py\n",
            "            ├── als.py\n",
            "            ├── avro_inputformat.py\n",
            "            ├── kmeans.py\n",
            "            ├── logistic_regression.py\n",
            "            ├── \u001b[01;34mml\u001b[0m\n",
            "            │   ├── aft_survival_regression.py\n",
            "            │   ├── als_example.py\n",
            "            │   ├── binarizer_example.py\n",
            "            │   ├── bisecting_k_means_example.py\n",
            "            │   ├── bucketed_random_projection_lsh_example.py\n",
            "            │   ├── bucketizer_example.py\n",
            "            │   ├── chi_square_test_example.py\n",
            "            │   ├── chisq_selector_example.py\n",
            "            │   ├── correlation_example.py\n",
            "            │   ├── count_vectorizer_example.py\n",
            "            │   ├── cross_validator.py\n",
            "            │   ├── dataframe_example.py\n",
            "            │   ├── dct_example.py\n",
            "            │   ├── decision_tree_classification_example.py\n",
            "            │   ├── decision_tree_regression_example.py\n",
            "            │   ├── elementwise_product_example.py\n",
            "            │   ├── estimator_transformer_param_example.py\n",
            "            │   ├── feature_hasher_example.py\n",
            "            │   ├── fm_classifier_example.py\n",
            "            │   ├── fm_regressor_example.py\n",
            "            │   ├── fpgrowth_example.py\n",
            "            │   ├── gaussian_mixture_example.py\n",
            "            │   ├── generalized_linear_regression_example.py\n",
            "            │   ├── gradient_boosted_tree_classifier_example.py\n",
            "            │   ├── gradient_boosted_tree_regressor_example.py\n",
            "            │   ├── imputer_example.py\n",
            "            │   ├── index_to_string_example.py\n",
            "            │   ├── interaction_example.py\n",
            "            │   ├── isotonic_regression_example.py\n",
            "            │   ├── kmeans_example.py\n",
            "            │   ├── lda_example.py\n",
            "            │   ├── linear_regression_with_elastic_net.py\n",
            "            │   ├── linearsvc.py\n",
            "            │   ├── logistic_regression_summary_example.py\n",
            "            │   ├── logistic_regression_with_elastic_net.py\n",
            "            │   ├── max_abs_scaler_example.py\n",
            "            │   ├── min_hash_lsh_example.py\n",
            "            │   ├── min_max_scaler_example.py\n",
            "            │   ├── multiclass_logistic_regression_with_elastic_net.py\n",
            "            │   ├── multilayer_perceptron_classification.py\n",
            "            │   ├── n_gram_example.py\n",
            "            │   ├── naive_bayes_example.py\n",
            "            │   ├── normalizer_example.py\n",
            "            │   ├── one_vs_rest_example.py\n",
            "            │   ├── onehot_encoder_example.py\n",
            "            │   ├── pca_example.py\n",
            "            │   ├── pipeline_example.py\n",
            "            │   ├── polynomial_expansion_example.py\n",
            "            │   ├── power_iteration_clustering_example.py\n",
            "            │   ├── prefixspan_example.py\n",
            "            │   ├── quantile_discretizer_example.py\n",
            "            │   ├── random_forest_classifier_example.py\n",
            "            │   ├── random_forest_regressor_example.py\n",
            "            │   ├── rformula_example.py\n",
            "            │   ├── robust_scaler_example.py\n",
            "            │   ├── sql_transformer.py\n",
            "            │   ├── standard_scaler_example.py\n",
            "            │   ├── stopwords_remover_example.py\n",
            "            │   ├── string_indexer_example.py\n",
            "            │   ├── summarizer_example.py\n",
            "            │   ├── tf_idf_example.py\n",
            "            │   ├── tokenizer_example.py\n",
            "            │   ├── train_validation_split.py\n",
            "            │   ├── univariate_feature_selector_example.py\n",
            "            │   ├── variance_threshold_selector_example.py\n",
            "            │   ├── vector_assembler_example.py\n",
            "            │   ├── vector_indexer_example.py\n",
            "            │   ├── vector_size_hint_example.py\n",
            "            │   ├── vector_slicer_example.py\n",
            "            │   └── word2vec_example.py\n",
            "            ├── \u001b[01;34mmllib\u001b[0m\n",
            "            │   ├── __init__.py\n",
            "            │   ├── binary_classification_metrics_example.py\n",
            "            │   ├── bisecting_k_means_example.py\n",
            "            │   ├── \u001b[01;32mcorrelations.py\u001b[0m\n",
            "            │   ├── correlations_example.py\n",
            "            │   ├── decision_tree_classification_example.py\n",
            "            │   ├── decision_tree_regression_example.py\n",
            "            │   ├── elementwise_product_example.py\n",
            "            │   ├── fpgrowth_example.py\n",
            "            │   ├── gaussian_mixture_example.py\n",
            "            │   ├── gaussian_mixture_model.py\n",
            "            │   ├── gradient_boosting_classification_example.py\n",
            "            │   ├── gradient_boosting_regression_example.py\n",
            "            │   ├── hypothesis_testing_example.py\n",
            "            │   ├── hypothesis_testing_kolmogorov_smirnov_test_example.py\n",
            "            │   ├── isotonic_regression_example.py\n",
            "            │   ├── k_means_example.py\n",
            "            │   ├── kernel_density_estimation_example.py\n",
            "            │   ├── \u001b[01;32mkmeans.py\u001b[0m\n",
            "            │   ├── latent_dirichlet_allocation_example.py\n",
            "            │   ├── linear_regression_with_sgd_example.py\n",
            "            │   ├── \u001b[01;32mlogistic_regression.py\u001b[0m\n",
            "            │   ├── logistic_regression_with_lbfgs_example.py\n",
            "            │   ├── multi_class_metrics_example.py\n",
            "            │   ├── multi_label_metrics_example.py\n",
            "            │   ├── naive_bayes_example.py\n",
            "            │   ├── normalizer_example.py\n",
            "            │   ├── pca_rowmatrix_example.py\n",
            "            │   ├── power_iteration_clustering_example.py\n",
            "            │   ├── random_forest_classification_example.py\n",
            "            │   ├── random_forest_regression_example.py\n",
            "            │   ├── \u001b[01;32mrandom_rdd_generation.py\u001b[0m\n",
            "            │   ├── ranking_metrics_example.py\n",
            "            │   ├── recommendation_example.py\n",
            "            │   ├── regression_metrics_example.py\n",
            "            │   ├── \u001b[01;32msampled_rdds.py\u001b[0m\n",
            "            │   ├── standard_scaler_example.py\n",
            "            │   ├── stratified_sampling_example.py\n",
            "            │   ├── streaming_k_means_example.py\n",
            "            │   ├── streaming_linear_regression_example.py\n",
            "            │   ├── summary_statistics_example.py\n",
            "            │   ├── svd_example.py\n",
            "            │   ├── svm_with_sgd_example.py\n",
            "            │   ├── tf_idf_example.py\n",
            "            │   ├── word2vec.py\n",
            "            │   └── word2vec_example.py\n",
            "            ├── pagerank.py\n",
            "            ├── parquet_inputformat.py\n",
            "            ├── pi.py\n",
            "            ├── sort.py\n",
            "            ├── \u001b[01;34msql\u001b[0m\n",
            "            │   ├── __init__.py\n",
            "            │   ├── arrow.py\n",
            "            │   ├── basic.py\n",
            "            │   ├── datasource.py\n",
            "            │   ├── hive.py\n",
            "            │   ├── \u001b[01;34mstreaming\u001b[0m\n",
            "            │   │   ├── structured_kafka_wordcount.py\n",
            "            │   │   ├── structured_network_wordcount.py\n",
            "            │   │   ├── structured_network_wordcount_session_window.py\n",
            "            │   │   ├── structured_network_wordcount_windowed.py\n",
            "            │   │   └── structured_sessionization.py\n",
            "            │   └── udtf.py\n",
            "            ├── status_api_demo.py\n",
            "            ├── \u001b[01;34mstreaming\u001b[0m\n",
            "            │   ├── __init__.py\n",
            "            │   ├── hdfs_wordcount.py\n",
            "            │   ├── network_wordcount.py\n",
            "            │   ├── network_wordjoinsentiments.py\n",
            "            │   ├── queue_stream.py\n",
            "            │   ├── recoverable_network_wordcount.py\n",
            "            │   ├── sql_network_wordcount.py\n",
            "            │   └── stateful_network_wordcount.py\n",
            "            ├── transitive_closure.py\n",
            "            └── wordcount.py\n",
            "\n",
            "8 directories, 147 files\n"
          ]
        }
      ],
      "source": [
        "# All examples\n",
        "!tree -I \"__pycache__\" $SPARK_HOME/examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBPlCZvmxEns",
        "outputId": "e7064d6a-6b93-4ad1-aa9b-45b0e4646db5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[01;34m/home/camilo/.local/share/virtualenvs/MMDS-OQFbRnK5/lib/python3.11/site-packages/pyspark/data\u001b[0m\n",
            "├── \u001b[01;34martifact-tests\u001b[0m\n",
            "│   └── \u001b[01;34mcrc\u001b[0m\n",
            "│       ├── junitLargeJar.txt\n",
            "│       └── smallJar.txt\n",
            "├── \u001b[01;34mgraphx\u001b[0m\n",
            "│   ├── followers.txt\n",
            "│   └── users.txt\n",
            "├── \u001b[01;34mmllib\u001b[0m\n",
            "│   ├── \u001b[01;34mals\u001b[0m\n",
            "│   │   ├── sample_movielens_ratings.txt\n",
            "│   │   └── test.data\n",
            "│   ├── gmm_data.txt\n",
            "│   ├── \u001b[01;34mimages\u001b[0m\n",
            "│   │   ├── license.txt\n",
            "│   │   └── \u001b[01;34morigin\u001b[0m\n",
            "│   │       ├── \u001b[01;34mkittens\u001b[0m\n",
            "│   │       │   └── not-image.txt\n",
            "│   │       └── license.txt\n",
            "│   ├── kmeans_data.txt\n",
            "│   ├── pagerank_data.txt\n",
            "│   ├── pic_data.txt\n",
            "│   ├── \u001b[01;34mridge-data\u001b[0m\n",
            "│   │   └── lpsa.data\n",
            "│   ├── sample_binary_classification_data.txt\n",
            "│   ├── sample_fpgrowth.txt\n",
            "│   ├── sample_isotonic_regression_libsvm_data.txt\n",
            "│   ├── sample_kmeans_data.txt\n",
            "│   ├── sample_lda_data.txt\n",
            "│   ├── sample_lda_libsvm_data.txt\n",
            "│   ├── sample_libsvm_data.txt\n",
            "│   ├── \u001b[01;32msample_linear_regression_data.txt\u001b[0m\n",
            "│   ├── sample_movielens_data.txt\n",
            "│   ├── sample_multiclass_classification_data.txt\n",
            "│   ├── sample_svm_data.txt\n",
            "│   └── streaming_kmeans_data_test.txt\n",
            "└── \u001b[01;34mstreaming\u001b[0m\n",
            "    └── AFINN-111.txt\n",
            "\n",
            "10 directories, 27 files\n"
          ]
        }
      ],
      "source": [
        "# All example datasets\n",
        "!tree $SPARK_HOME/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5HNL29Ju3K6"
      },
      "source": [
        "## Ejemplo2: Contar palabras\n",
        "Ya vimos que PySpark trae ejemplos incluyendo wordcount.py, pero no tenemos un dataset decente para texto. Descarguemos Don Quijote para analizarlo y hagamos nuestro propio contador de palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cm33nRhoyKaY",
        "outputId": "d14d3878-72f4-415a-f4e4-4a61c44755af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-02-20 10:47:43--  https://www.gutenberg.org/cache/epub/996/pg996.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2391728 (2.3M) [text/plain]\n",
            "Saving to: ‘don_quixote.txt’\n",
            "\n",
            "don_quixote.txt     100%[===================>]   2.28M  3.84MB/s    in 0.6s    \n",
            "\n",
            "2024-02-20 10:47:44 (3.84 MB/s) - ‘don_quixote.txt’ saved [2391728/2391728]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.gutenberg.org/cache/epub/996/pg996.txt -O don_quixote.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiVbhPHTyRoG",
        "outputId": "f65d3a18-ce2f-4424-9e54-00c9bcd5f571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Project Gutenberg eBook of Don Quixote\n",
            "    \n",
            "This ebook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "of the Project Gutenberg License included with this ebook or online\n",
            "at www.gutenberg.org. If you are not located in the United States,\n",
            "you will have to check the laws of the country where you are located\n",
            "before using this eBook.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!head -10 don_quixote.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "AsNLZqrjy6PD"
      },
      "outputs": [],
      "source": [
        "# Copy into current folder\n",
        "!cp $SPARK_HOME/examples/src/main/python/wordcount.py ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXxqVBbl0Cd_",
        "outputId": "b4a36be8-9946-4d95-9429-1804b319b53a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "import sys\n",
            "from operator import add\n",
            "\n",
            "from pyspark.sql import SparkSession\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    if len(sys.argv) != 2:\n",
            "        print(\"Usage: wordcount <file>\", file=sys.stderr)\n",
            "        sys.exit(-1)\n",
            "\n",
            "    spark = SparkSession\\\n",
            "        .builder\\\n",
            "        .appName(\"PythonWordCount\")\\\n",
            "        .getOrCreate()\n",
            "\n",
            "    lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\n",
            "    counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
            "                  .map(lambda x: (x, 1)) \\\n",
            "                  .reduceByKey(add)\n",
            "    output = counts.collect()\n",
            "    for (word, count) in output:\n",
            "        print(\"%s: %i\" % (word, count))\n",
            "\n",
            "    spark.stop()\n"
          ]
        }
      ],
      "source": [
        "# wordcount.py but without comments\n",
        "!sed -n 18,42p wordcount.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "R-3tqmIuzFCJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 159 ms, sys: 40.3 ms, total: 200 ms\n",
            "Wall time: 10.4 s\n"
          ]
        }
      ],
      "source": [
        "# Run wordcount.py with output (1: standard output) and error (2: error output) files\n",
        "%time !spark-submit wordcount.py don_quixote.txt 1>out.txt 2>err.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LdlbW0szbh7",
        "outputId": "241839ee-1ee5-4fbd-a6e0-781f0c7adbab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The: 846\n",
            "Project: 80\n",
            "Gutenberg: 23\n",
            "eBook: 4\n",
            "of: 12866\n",
            "Don: 2541\n",
            "Quixote: 1012\n",
            ": 8413\n",
            "This: 97\n",
            "ebook: 2\n"
          ]
        }
      ],
      "source": [
        "!head out.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO33qLRj3rGx"
      },
      "source": [
        "Para trabajar de manera interactiva puedes usar Python directamente en el cuaderno o ejecutar los scripts con el comando `python`, pero toda la configuración de logs y demás variables de ambiente en Spark serán ignoradas (`spark-submit` se encarga de configurar las variables de ambiente de Spark).\n",
        "\n",
        "De todas maneras hagamos una prueba:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5PuA7MEH4LZu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "your 131072x1 screen size is bogus. expect trouble\n",
            "24/02/20 10:58:50 WARN Utils: Your hostname, Camilo resolves to a loopback address: 127.0.1.1; using 172.29.121.68 instead (on interface eth0)\n",
            "24/02/20 10:58:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "24/02/20 10:58:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "spark = SparkSession.builder.appName('PythonWordCount').getOrCreate()\n",
        "lines = spark.read.text('don_quixote.txt').rdd.map(lambda r: r[0])\n",
        "counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
        "                  .map(lambda x: (x, 1)) \\\n",
        "                  .reduceByKey(add)\n",
        "output = counts.collect()\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MMZ6j096X6y",
        "outputId": "372e5217-1a9e-480a-e7b5-54b5fed0143a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('The', 846),\n",
              " ('Project', 80),\n",
              " ('Gutenberg', 23),\n",
              " ('eBook', 4),\n",
              " ('of', 12866),\n",
              " ('Don', 2541),\n",
              " ('Quixote', 1012),\n",
              " ('', 8413),\n",
              " ('This', 97),\n",
              " ('ebook', 2),\n",
              " ('is', 3504),\n",
              " ('for', 4535),\n",
              " ('the', 20933),\n",
              " ('use', 64),\n",
              " ('anyone', 82),\n",
              " ('anywhere', 10),\n",
              " ('in', 6864),\n",
              " ('United', 15),\n",
              " ('States', 8),\n",
              " ('and', 16604)]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRaF2A_j_nC7"
      },
      "source": [
        "## MISIÓN: Contar palabras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebLNUxP0_8x3"
      },
      "source": [
        "Tu misión si decides aceptarla es cambiar el contador de palabras para que te muestre cuántas palabras comienzan por cada letra, ignorando mayúsculas y minúsculas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "PttUSz3B7MFa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "spark = SparkSession.builder.appName('PythonWordCount').getOrCreate()\n",
        "lines = spark.read.text('don_quixote.txt').rdd.map(lambda r: r[0])\n",
        "counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
        "                  .filter(lambda x: x and x[0]) \\\n",
        "                  .map(lambda x: (x[0].lower(), 1)) \\\n",
        "                  .reduceByKey(add)\n",
        "output = sorted(counts.collect())\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('#', 1),\n",
              " ('$', 1),\n",
              " ('&', 1),\n",
              " ('(', 570),\n",
              " ('*', 4),\n",
              " ('-', 3),\n",
              " ('.', 1),\n",
              " ('1', 90),\n",
              " ('2', 10),\n",
              " ('3', 5),\n",
              " ('4', 3),\n",
              " ('5', 5),\n",
              " ('6', 3),\n",
              " ('7', 1),\n",
              " ('8', 2),\n",
              " ('9', 3),\n",
              " ('[', 1),\n",
              " ('_', 84),\n",
              " ('a', 49404),\n",
              " ('b', 18470),\n",
              " ('c', 14799),\n",
              " ('d', 14157),\n",
              " ('e', 6803),\n",
              " ('f', 16010),\n",
              " ('g', 8638),\n",
              " ('h', 32160),\n",
              " ('i', 28519),\n",
              " ('j', 960),\n",
              " ('k', 3256),\n",
              " ('l', 10090),\n",
              " ('m', 18921),\n",
              " ('n', 9470),\n",
              " ('o', 27047),\n",
              " ('p', 10890),\n",
              " ('q', 3104),\n",
              " ('r', 8265),\n",
              " ('s', 30620),\n",
              " ('t', 70652),\n",
              " ('u', 4277),\n",
              " ('v', 2595)]"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output[:40]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "spark = SparkSession.builder.appName('PythonWordCount').getOrCreate()\n",
        "lines = spark.read.text('don_quixote.txt').rdd.map(lambda r: r[0])\n",
        "counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
        "                  .map(lambda x: x.replace(',', '')\n",
        "                       .replace('#', '')\n",
        "                       .replace('$', '')\n",
        "                       .replace('&', '')\n",
        "                       .replace('(', '')\n",
        "                       .replace('*', '')\n",
        "                       .replace('-', '')\n",
        "                       .replace('.', '')\n",
        "                       .replace('1', '')\n",
        "                       .replace('2', '')\n",
        "                       .replace('3', '')\n",
        "                       .replace('4', '')\n",
        "                       .replace('5', '')\n",
        "                       .replace('6', '')\n",
        "                       .replace('7', '')\n",
        "                       .replace('[', '')\n",
        "                       .replace('_', '')\n",
        "                       .replace('\"', '')\n",
        "                       .replace('[', '')                    \n",
        "                       .replace('@', '')) \\\n",
        "                  .filter(lambda x: x and x[0].isalpha() and x[0].isascii()) \\\n",
        "                  .map(lambda x: (x[0].lower(), 1)) \\\n",
        "                  .map(lambda x: (x)) \\\n",
        "                  .reduceByKey(add)\n",
        "output = sorted(counts.collect())\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('a', 49458),\n",
              " ('b', 18482),\n",
              " ('c', 14805),\n",
              " ('d', 14167),\n",
              " ('e', 6829),\n",
              " ('f', 16087),\n",
              " ('g', 8641),\n",
              " ('h', 32163),\n",
              " ('i', 28536),\n",
              " ('j', 962),\n",
              " ('k', 3468),\n",
              " ('l', 10096),\n",
              " ('m', 18929),\n",
              " ('n', 9476),\n",
              " ('o', 27060),\n",
              " ('p', 10903),\n",
              " ('q', 3106),\n",
              " ('r', 8270),\n",
              " ('s', 30631),\n",
              " ('t', 70669),\n",
              " ('u', 4278),\n",
              " ('v', 2596),\n",
              " ('w', 29797),\n",
              " ('x', 166),\n",
              " ('y', 4531),\n",
              " ('z', 111),\n",
              " ('à', 1),\n",
              " ('á', 21),\n",
              " ('æ', 11),\n",
              " ('é', 1),\n",
              " ('í', 1),\n",
              " ('ú', 6)]"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "spark = SparkSession.builder.appName('PythonWordCount').getOrCreate()\n",
        "lines = spark.read.text('don_quixote.txt').rdd.map(lambda r: r[0])\n",
        "counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
        "                  .filter(lambda x: x and x[0].isalpha() and x[0].isascii()) \\\n",
        "                  .map(lambda x: (x[0].lower(), 1)) \\\n",
        "                  .map(lambda x: (x)) \\\n",
        "                  .reduceByKey(add)\n",
        "output = sorted(counts.collect())\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('a', 49404),\n",
              " ('b', 18470),\n",
              " ('c', 14799),\n",
              " ('d', 14157),\n",
              " ('e', 6803),\n",
              " ('f', 16010),\n",
              " ('g', 8638),\n",
              " ('h', 32160),\n",
              " ('i', 28519),\n",
              " ('j', 960),\n",
              " ('k', 3256),\n",
              " ('l', 10090),\n",
              " ('m', 18921),\n",
              " ('n', 9470),\n",
              " ('o', 27047),\n",
              " ('p', 10890),\n",
              " ('q', 3104),\n",
              " ('r', 8265),\n",
              " ('s', 30620),\n",
              " ('t', 70652),\n",
              " ('u', 4277),\n",
              " ('v', 2595),\n",
              " ('w', 29767),\n",
              " ('x', 166),\n",
              " ('y', 4531),\n",
              " ('z', 111)]"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
